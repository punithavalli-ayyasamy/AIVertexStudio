{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgriAutoML Pipeline Execution in Vertex AI\n",
    "\n",
    "This notebook demonstrates how to execute the AgriAutoML pipeline directly in Vertex AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip cache purge\n",
    "\n",
    "# First, uninstall all related packages\n",
    "%pip uninstall -y google-cloud-aiplatform google-cloud-storage google-cloud-datastore protobuf google-cloud-bigquery google-genai kfp pydantic\n",
    "\n",
    "\n",
    "%pip install --no-cache-dir \"protobuf>=4.21.1,<5.0.0\"\n",
    "%pip install --no-cache-dir \"pydantic\"\n",
    "\n",
    "%pip install --no-cache-dir \"google-cloud-bigquery<3.0.0\"\n",
    "#%pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Install core dependencies first\n",
    "\n",
    "%pip install --no-cache-dir \"setuptools>=65.5.1\"\n",
    "%pip install --no-cache-dir \"wheel>=0.40.0\"\n",
    "\n",
    "# Try installing KFP 2.0.1 specifically (a stable version)\n",
    "%pip install --no-cache-dir kfp>=2.0.0 --use-pep517\n",
    "\n",
    "\n",
    "# Install remaining dependencies after KFP is installed\n",
    "%pip install --no-cache-dir \"google-cloud-storage>=1.32.0,<3.0.0\"\n",
    "%pip install --no-cache-dir \"google-cloud-datastore==1.15.5\"\n",
    "%pip install --no-cache-dir \"google-cloud-aiplatform==1.104.0\"\n",
    "%pip install --no-cache-dir \"google-cloud-bigquery<3.0.0\"\n",
    "%pip install --no-cache-dir pandas numpy pillow scikit-learn tensorflow\n",
    "%pip install --no-cache-dir google-auth google-auth-httplib2 google-api-python-client\n",
    "\n",
    "\n",
    "\n",
    "%pip install --no-cache-dir   'click>=8.0.0,<9.0.0\"\n",
    "%pip install --no-cache-dir    'kfp-pipeline-spec==0.6.0\"\n",
    "%pip install --no-cache-dir    'kfp-server-api>=2.1.0,<2.5.0\"\n",
    "%pip install --no-cache-dir    'kubernetes>=8.0.0,<31.0.0\"\n",
    "%pip install --no-cache-dir    'PyYAML>=5.3,<7.0.0\"\n",
    "%pip install --no-cache-dir    'requests-toolbelt>=0.8.0,<2.0.0\"\n",
    "%pip install --no-cache-dir    'tabulate>=0.8.6,<1.0.0\"\n",
    "\n",
    "\n",
    "# Install remaining dependencies\n",
    "%pip install pandas numpy pillow scikit-learn tensorflow google-auth google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "from datetime import datetime\n",
    "from kfp import dsl,compiler\n",
    "from kfp.dsl import Output, Dataset, Input, Artifact\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import io\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "# Get default credentials and project\n",
    "credentials, project_id = default()\n",
    "\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "bucket_name = \"agrifingcpflow-465809-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{bucket_name}/pipeline_root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample datasets for vision and tabular models.\"\"\"\n",
    "    # Create sample vision data (dummy image)\n",
    "    vision_uri = f\"gs://{bucket_name}/sample_vision_data\"\n",
    "    \n",
    "    # Create sample tabular data\n",
    "    tabular_uri = f\"gs://{bucket_name}/sample_tabular_data\"\n",
    "    \n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Create and upload sample vision data\n",
    "    img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    \n",
    "    vision_blob = bucket.blob('sample_vision_data/image1.png')\n",
    "    vision_blob.upload_from_string(img_byte_arr, content_type='image/png')\n",
    "    \n",
    "    # Create and upload sample tabular data\n",
    "    df = pd.DataFrame({\n",
    "        'planting_date': pd.date_range(start='2025-01-01', periods=100),\n",
    "        'temperature': np.random.normal(25, 5, 100),\n",
    "        'rainfall': np.random.normal(50, 10, 100),\n",
    "        'soil_quality': np.random.choice(['good', 'medium', 'poor'], 100),\n",
    "        'yield': np.random.normal(75, 15, 100)\n",
    "    })\n",
    "    \n",
    "    tabular_blob = bucket.blob('sample_tabular_data/farming_data.csv')\n",
    "    tabular_blob.upload_from_string(df.to_csv(index=False))\n",
    "    \n",
    "    return vision_uri, tabular_uri\n",
    "\n",
    "# Create the sample data and get the URIs\n",
    "vision_uri, tabular_uri = create_sample_data()\n",
    "print(f\"Created vision dataset at: {vision_uri}\")\n",
    "print(f\"Created tabular dataset at: {tabular_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'google-cloud-aiplatform==1.104.0',\n",
    "        'pandas',\n",
    "        'pillow',\n",
    "        'numpy'\n",
    "    ]\n",
    ")\n",
    "def preprocess_data(\n",
    "    vision_data: str,\n",
    "    tabular_data: str,\n",
    "    bucket_name: str,\n",
    "    project_id: str,  # Add project_id as parameter\n",
    "    region: str,    # Add region as parameter\n",
    "    vision_dataset: Output[Artifact],\n",
    "    tabular_dataset: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess vision and tabular data for training\n",
    "    \n",
    "    Args:\n",
    "        vision_data: GCS URI for vision dataset\n",
    "        tabular_data: GCS URI for tabular dataset\n",
    "        bucket_name: GCS bucket for processed data\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        vision_dataset: Output path for processed vision data\n",
    "        tabular_dataset: Output path for processed tabular data\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "       \n",
    "       \n",
    "    # Configure logging to output to both file and console\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(sys.stdout),  # Console output\n",
    "            logging.FileHandler('preprocess.log')  # File output\n",
    "        ]\n",
    "    )\n",
    "    log = logging.getLogger('preprocess_data')\n",
    "    # Initialize GCS client\n",
    "    log.info(f\"Initializing GCS client for project {project_id} in {region}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "   # Process vision data\n",
    "    def process_image(image_bytes):\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((224, 224))  # Standard size for many vision models\n",
    "        return np.array(img)\n",
    "\n",
    "     # Process tabular data\n",
    "    def process_tabular(df):\n",
    "        # Handle missing values\n",
    "        df = df.fillna(df.mean())\n",
    "        \n",
    "        # Feature engineering\n",
    "        if \"planting_date\" in df.columns:\n",
    "            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"])\n",
    "            df[\"planting_month\"] = df[\"planting_date\"].dt.month\n",
    "            df[\"planting_day\"] = df[\"planting_date\"].dt.day\n",
    "        \n",
    "        return df\n",
    "   \n",
    "    log.info(\"Processing and uploading vision data\")\n",
    "    # Process and save datasets\n",
    "    vision_blob = bucket.blob('processed_vision_data.txt')\n",
    "    vision_blob.upload_from_string(vision_data)\n",
    "    vision_output_uri = f\"gs://{bucket_name}/{vision_blob.name}\"\n",
    "\n",
    "     # Save to the KFP output location\n",
    "    with open(vision_dataset.path, 'w') as f:\n",
    "        f.write(vision_output_uri)\n",
    "\n",
    "    tabular_blob = bucket.blob('processed_tabular_data.csv')\n",
    "    tabular_blob.upload_from_string(tabular_data)\n",
    "    tabular_output_uri = f\"gs://{bucket_name}/{tabular_blob.name}\"\n",
    "    \n",
    "    # Save to the KFP output location\n",
    "    with open(tabular_dataset.path, 'w') as f:\n",
    "        f.write(tabular_output_uri)\n",
    "    log.info(\"Preprocessing completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform>=1.25.0',\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'tensorflow'  # If you're using TF-based vision models\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_vision_model(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    dataset: Input[Artifact],\n",
    "    min_accuracy: float,\n",
    "    model_info: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Train AutoML Vision model for crop analysis\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: Input artifact containing the processed vision dataset URI\n",
    "        min_accuracy: Minimum required accuracy\n",
    "        model_info: Output artifact for model information\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    import json\n",
    "    import vertex_ai as aiplatform\n",
    "    \n",
    "    # Configure logging to output to both file and console\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(sys.stdout),  # Console output\n",
    "            logging.FileHandler('train_vision_model.log')  # File output\n",
    "        ]\n",
    "    )\n",
    "    log = logging.getLogger('train_vision_model')\n",
    "\n",
    "     # Read dataset URI from input artifact\n",
    "    with open(dataset.path, 'r') as f:\n",
    "        dataset_uri = f.read().strip()\n",
    "    log.info(\"Initialize Vertex AI\")\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    log.info(\"Create dataset\")\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.ImageDataset.create(\n",
    "        display_name=\"crop_vision_dataset\",\n",
    "        gcs_source=dataset_uri\n",
    "    )\n",
    "    log.info(\"Train model\")\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLImageTrainingJob(\n",
    "        display_name=\"crop_vision_model\",\n",
    "        prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=83,  # Approximately 5 minutes\n",
    "        model_type=\"CLOUD\",\n",
    "        base_model=None\n",
    "    )\n",
    "    log.info(\"training job\")\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        budget_milli_node_hours=83,  # 5 minutes for testing\n",
    "        training_filter_split=\"\",  # No filter\n",
    "        model_display_name=\"crop_vision_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1\n",
    "    )\n",
    "    log.info(\"Get model evaluation\")\n",
    "    # Get model evaluation\n",
    "    eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "    log.info(\"Check if model meets accuracy threshold\")\n",
    "    # Check if model meets accuracy threshold\n",
    "    if eval_metrics.metrics['auRoc'] < min_accuracy:\n",
    "        raise ValueError(f\"Model accuracy {eval_metrics.metrics['auRoc']} below threshold {min_accuracy}\")\n",
    "    log.info(\"Return model info\")\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'accuracy': float(eval_metrics.metrics['auRoc'])\n",
    "    }\n",
    "    with open(model_info.path, 'w') as f:\n",
    "        json.dump(info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "         'google-cloud-aiplatform>=1.25.0',\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'xgboost',\n",
    "        'lightgbm',\n",
    "        'protobuf>=4.21.1,<5.0.0',  # Pin protobuf version for KFP compatibility\n",
    "        'click>=8.0.0,<9.0.0',\n",
    "        'kfp-pipeline-spec==0.6.0',\n",
    "        'kfp-server-api>=2.1.0,<2.5.0',\n",
    "        'kubernetes>=8.0.0,<31.0.0',\n",
    "        'PyYAML>=5.3,<7.0.0',\n",
    "        'requests-toolbelt>=0.8.0,<2.0.0',\n",
    "        'tabulate>=0.8.6,<1.0.0'\n",
    "    ]\n",
    ")\n",
    "def train_tabular_model(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    dataset: Input[Artifact],\n",
    "    min_accuracy: float,\n",
    "    model_info: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Train AutoML Tabular model for crop yield prediction\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: Input artifact containing the processed tabular dataset URI\n",
    "        min_accuracy: Minimum required accuracy (RMSE threshold)\n",
    "        model_info: Output artifact for model information\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from google.cloud import aiplatform\n",
    "    import sys\n",
    "    \n",
    "    \n",
    "     # Read dataset URI from input artifact\n",
    "    with open(dataset.path, 'r') as f:\n",
    "        dataset_uri = f.read().strip()\n",
    "\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.TabularDataset.create(\n",
    "        display_name=\"crop_tabular_dataset\",\n",
    "        gcs_source=dataset_uri\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name=\"crop_tabular_model\",\n",
    "        optimization_objective=\"minimize-rmse\",\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"field_size\"}},\n",
    "            {\"numeric\": {\"column_name\": \"rainfall\"}},\n",
    "            {\"numeric\": {\"column_name\": \"temperature\"}},\n",
    "            {\"categorical\": {\"column_name\": \"location\"}},\n",
    "            {\"categorical\": {\"column_name\": \"crop_type\"}},\n",
    "            {\"timestamp\": {\"column_name\": \"date\"}}\n",
    "        ],\n",
    "        target_column=\"yield\",\n",
    "        budget_milli_node_hours=83,  # Approximately 5 minutes\n",
    "        optimization_prediction_type=\"regression\",\n",
    "        additional_experiments=[\"enable_model_compression\"]\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        model_display_name=\"crop_yield_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1\n",
    "    )\n",
    "\n",
    "    # Get model evaluation\n",
    "    eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "\n",
    "    # Check if model meets accuracy threshold\n",
    "    if eval_metrics.metrics['rmse'] > min_accuracy:\n",
    "        raise ValueError(f\"Model RMSE {eval_metrics.metrics['rmse']} above threshold {min_accuracy}\")\n",
    "\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'rmse': float(eval_metrics.metrics['rmse'])\n",
    "    }\n",
    "    with open(model_info.path, 'w') as f:\n",
    "        json.dump(info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform>=1.25.0',\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'protobuf<4.0.0dev,>=3.19.5'  # Often needed for compatibility\n",
    "    ]\n",
    ")\n",
    "def deploy_models(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    vision_model: Input[Artifact],\n",
    "    tabular_model: Input[Artifact],\n",
    "    endpoints: Output[Artifact]\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploy trained models to endpoints\n",
    "    \n",
    "     Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        vision_model: Input artifact containing vision model information\n",
    "        tabular_model: Input artifact containing tabular model information\n",
    "        endpoints: Output artifact for endpoint information\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import Model\n",
    "    import time\n",
    "\n",
    "     # Read model info from input artifacts\n",
    "    with open(vision_model.path, 'r') as f:\n",
    "        vision_model_info = json.load(f)\n",
    "    \n",
    "    with open(tabular_model.path, 'r') as f:\n",
    "        tabular_model_info = json.load(f)\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Deploy vision model\n",
    "    vision_model_resource = aiplatform.Model(vision_model_info['model'])\n",
    "    vision_endpoint = vision_model_resource.deploy(\n",
    "        machine_type='e2-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    # Deploy tabular model\n",
    "    tabular_model_resource = aiplatform.Model(tabular_model_info['model'])\n",
    "    tabular_endpoint = tabular_model_resource.deploy(\n",
    "        machine_type='e2-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    # Write endpoint information to output artifact\n",
    "    endpoint_info = {\n",
    "        'vision_endpoint': vision_endpoint.resource_name,\n",
    "        'tabular_endpoint': tabular_endpoint.resource_name\n",
    "    }\n",
    "    \n",
    "    with open(endpoints.path, 'w') as f:\n",
    "        json.dump(endpoint_info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "@dsl.pipeline(\n",
    "    name='AgriAutoML Pipeline',\n",
    "    description='End-to-end pipeline for agricultural yield prediction'\n",
    ")\n",
    "def agri_automl_pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    vision_dataset_uri: str,\n",
    "    tabular_dataset_uri: str,\n",
    "    min_accuracy: float = 0.8\n",
    "):\n",
    "    # Preprocess data\n",
    "    preprocess_task = preprocess_data(\n",
    "        vision_data=vision_dataset_uri,\n",
    "        tabular_data=tabular_dataset_uri,\n",
    "        bucket_name=bucket_name,\n",
    "        project_id=project_id,  # Add this\n",
    "        region=region \n",
    "    )\n",
    "\n",
    "    # Train vision model\n",
    "    train_vision_task = train_vision_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['vision_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_vision_task.after(preprocess_task)\n",
    "\n",
    "    # Train tabular model\n",
    "    train_tabular_task = train_tabular_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['tabular_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_tabular_task.after(preprocess_task)\n",
    "\n",
    "    # Deploy models\n",
    "    deploy_task = deploy_models(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        vision_model=train_vision_task.outputs['model_info'],\n",
    "        tabular_model=train_tabular_task.outputs['model_info']\n",
    "    )\n",
    "    deploy_task.after(train_vision_task, train_tabular_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GCP setup\n",
    "print(f\"Current Project ID: {project_id}\")\n",
    "print(f\"Current Region: {REGION}\")\n",
    "print(\"Authenticated as:\", credentials.service_account_email if hasattr(credentials, 'service_account_email') else \"User Account\")\n",
    "\n",
    "# Test GCP API access\n",
    "storage_client = storage.Client()\n",
    "try:\n",
    "    # List buckets to test access\n",
    "    buckets = list(storage_client.list_buckets(max_results=1))\n",
    "    print(\"✅ Storage API access successful\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Storage API access failed:\", str(e))\n",
    "\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "\n",
    "# Compile pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=agri_automl_pipeline,\n",
    "    package_path='pipeline.yaml'\n",
    ")\n",
    "\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name='agri-automl-pipeline',\n",
    "    template_path='pipeline.yaml',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,  # Changed from PROJECT_ID\n",
    "        'region': REGION,\n",
    "        'bucket_name': bucket_name,\n",
    "        'vision_dataset_uri': vision_uri,  # Changed from VISION_DATASET_URI\n",
    "        'tabular_dataset_uri': tabular_uri,  # Changed from TABULAR_DATASET_URI\n",
    "        'min_accuracy': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
