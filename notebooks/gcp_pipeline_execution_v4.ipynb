{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgriAutoML Pipeline Execution in Vertex AI\n",
    "\n",
    "This notebook demonstrates how to execute the AgriAutoML pipeline directly in Vertex AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, uninstall all related packages\n",
    "%pip uninstall -y google-cloud-aiplatform google-cloud-storage google-cloud-datastore protobuf google-cloud-bigquery google-genai kfp pydantic\n",
    "\n",
    "# Install packages with exact versions\n",
    "%pip install \"protobuf>=3.20.2,<4.0.0dev\"\n",
    "%pip install \"google-cloud-storage>=1.32.0,<3.0.0\"\n",
    "%pip install \"google-cloud-datastore==1.15.5\"\n",
    "%pip install \"google-cloud-aiplatform==1.104.0\"\n",
    "%pip install \"google-cloud-bigquery<3.0.0\"\n",
    "%pip install \"pydantic>=2.0.0,<3.0.0\"\n",
    "%pip install \"kfp>=2.0.0\"\n",
    "%pip install \"google-genai>=1.25.0\"\n",
    "\n",
    "# Install remaining dependencies\n",
    "%pip install pandas numpy pillow scikit-learn tensorflow google-auth google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "from datetime import datetime\n",
    "from kfp import dsl, components, compiler\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Get default credentials and project\n",
    "credentials, project_id = default()\n",
    "\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"qwiklabs-gcp-01-d4f6611afd55-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample datasets for vision and tabular models.\"\"\"\n",
    "    # Create sample vision data (dummy image)\n",
    "    vision_uri = f\"gs://{BUCKET_NAME}/sample_vision_data\"\n",
    "    \n",
    "    # Create sample tabular data\n",
    "    tabular_uri = f\"gs://{BUCKET_NAME}/sample_tabular_data\"\n",
    "    \n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    \n",
    "    # Create and upload sample vision data\n",
    "    img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    \n",
    "    vision_blob = bucket.blob('sample_vision_data/image1.png')\n",
    "    vision_blob.upload_from_string(img_byte_arr, content_type='image/png')\n",
    "    \n",
    "    # Create and upload sample tabular data\n",
    "    df = pd.DataFrame({\n",
    "        'planting_date': pd.date_range(start='2025-01-01', periods=100),\n",
    "        'temperature': np.random.normal(25, 5, 100),\n",
    "        'rainfall': np.random.normal(50, 10, 100),\n",
    "        'soil_quality': np.random.choice(['good', 'medium', 'poor'], 100),\n",
    "        'yield': np.random.normal(75, 15, 100)\n",
    "    })\n",
    "    \n",
    "    tabular_blob = bucket.blob('sample_tabular_data/farming_data.csv')\n",
    "    tabular_blob.upload_from_string(df.to_csv(index=False))\n",
    "    \n",
    "    return vision_uri, tabular_uri\n",
    "\n",
    "# Create the sample data and get the URIs\n",
    "vision_uri, tabular_uri = create_sample_data()\n",
    "print(f\"Created vision dataset at: {vision_uri}\")\n",
    "print(f\"Created tabular dataset at: {tabular_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(vision_data: str, tabular_data: str, bucket_name: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Preprocess vision and tabular data for training\n",
    "    \n",
    "    Args:\n",
    "        vision_data: GCS URI for vision dataset\n",
    "        tabular_data: GCS URI for tabular dataset\n",
    "        bucket_name: GCS bucket for processed data\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vision_dataset_uri, tabular_dataset_uri)\n",
    "    \"\"\"\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Process vision data\n",
    "    def process_image(image_bytes):\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((224, 224))  # Standard size for many vision models\n",
    "        return np.array(img)\n",
    "\n",
    "    # Process tabular data\n",
    "    def process_tabular(df):\n",
    "        # Handle missing values\n",
    "        df = df.fillna(df.mean())\n",
    "        \n",
    "        # Feature engineering\n",
    "        if \"planting_date\" in df.columns:\n",
    "            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"])\n",
    "            df[\"planting_month\"] = df[\"planting_date\"].dt.month\n",
    "            df[\"planting_day\"] = df[\"planting_date\"].dt.day\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # Process and save datasets\n",
    "    vision_blob = bucket.blob('processed_vision_data.txt')\n",
    "    vision_blob.upload_from_string(vision_data)\n",
    "    vision_output_uri = f\"gs://{bucket_name}/{vision_blob.name}\"\n",
    "\n",
    "    tabular_blob = bucket.blob('processed_tabular_data.csv')\n",
    "    tabular_blob.upload_from_string(tabular_data)\n",
    "    tabular_output_uri = f\"gs://{bucket_name}/{tabular_blob.name}\"\n",
    "    \n",
    "    return vision_output_uri, tabular_output_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vision_model(project_id: str, region: str, dataset: str, min_accuracy: float) -> dict:\n",
    "    \"\"\"\n",
    "    Train AutoML Vision model for crop analysis\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: URI of the processed vision dataset\n",
    "        min_accuracy: Minimum required accuracy\n",
    "        \n",
    "    Returns:\n",
    "        dict: Model information including resource name and metrics\n",
    "    \"\"\"\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.ImageDataset.create(\n",
    "        display_name=\"crop_vision_dataset\",\n",
    "        gcs_source=dataset\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLImageTrainingJob(\n",
    "        display_name=\"crop_vision_model\",\n",
    "        prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=83,  # Approximately 5 minutes\n",
    "        model_type=\"CLOUD\",\n",
    "        base_model=None\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        budget_milli_node_hours=83,  # 5 minutes for testing\n",
    "        training_filter_split=\"\",  # No filter\n",
    "        model_display_name=\"crop_vision_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1\n",
    "    )\n",
    "\n",
    "    # Get model evaluation\n",
    "    eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "\n",
    "    # Check if model meets accuracy threshold\n",
    "    if eval_metrics.metrics['auRoc'] < min_accuracy:\n",
    "        raise ValueError(f\"Model accuracy {eval_metrics.metrics['auRoc']} below threshold {min_accuracy}\")\n",
    "\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'accuracy': float(eval_metrics.metrics['auRoc'])\n",
    "    }\n",
    "    return model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabular_model(project_id: str, region: str, dataset: str, min_accuracy: float) -> dict:\n",
    "    \"\"\"\n",
    "    Train AutoML Tabular model for crop yield prediction\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: URI of the processed tabular dataset\n",
    "        min_accuracy: Minimum required accuracy (RMSE threshold)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Model information including resource name and metrics\n",
    "    \"\"\"\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.TabularDataset.create(\n",
    "        display_name=\"crop_tabular_dataset\",\n",
    "        gcs_source=dataset\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name=\"crop_tabular_model\",\n",
    "        optimization_objective=\"minimize-rmse\",\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"field_size\"}},\n",
    "            {\"numeric\": {\"column_name\": \"rainfall\"}},\n",
    "            {\"numeric\": {\"column_name\": \"temperature\"}},\n",
    "            {\"categorical\": {\"column_name\": \"location\"}},\n",
    "            {\"categorical\": {\"column_name\": \"crop_type\"}},\n",
    "            {\"timestamp\": {\"column_name\": \"date\"}}\n",
    "        ],\n",
    "        target_column=\"yield\",\n",
    "        budget_milli_node_hours=83,  # Approximately 5 minutes\n",
    "        optimization_prediction_type=\"regression\",\n",
    "        additional_experiments=[\"enable_model_compression\"]\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        model_display_name=\"crop_yield_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1\n",
    "    )\n",
    "\n",
    "    # Get model evaluation\n",
    "    eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "\n",
    "    # Check if model meets accuracy threshold\n",
    "    if eval_metrics.metrics['rmse'] > min_accuracy:\n",
    "        raise ValueError(f\"Model RMSE {eval_metrics.metrics['rmse']} above threshold {min_accuracy}\")\n",
    "\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'rmse': float(eval_metrics.metrics['rmse'])\n",
    "    }\n",
    "    return model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_models(project_id: str, region: str, vision_model: dict, tabular_model: dict) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Deploy trained models to endpoints\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        vision_model: Vision model information\n",
    "        tabular_model: Tabular model information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vision_endpoint_name, tabular_endpoint_name)\n",
    "    \"\"\"\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Deploy vision model\n",
    "    vision_model_resource = aiplatform.Model(vision_model['model'])\n",
    "    vision_endpoint = vision_model_resource.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    # Deploy tabular model\n",
    "    tabular_model_resource = aiplatform.Model(tabular_model['model'])\n",
    "    tabular_endpoint = tabular_model_resource.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    return vision_endpoint.resource_name, tabular_endpoint.resource_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the absolute path to the components directory\n",
    "COMPONENTS_DIR = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'components'))\n",
    "\n",
    "# Function to get absolute component path\n",
    "def get_component_path(component_name):\n",
    "    return os.path.join(os.path.dirname(os.getcwd()), 'components', component_name)\n",
    "    \n",
    "# Load components\n",
    "#preprocess_op = components.load_component_from_file(get_component_path('preprocess.yaml'))\n",
    "#train_vision_op = components.load_component_from_file(get_component_path('train_vision.yaml'))\n",
    "#train_tabular_op = components.load_component_from_file(get_component_path('train_tabular.yaml'))\n",
    "#deploy_op = components.load_component_from_file(get_component_path('deploy.yaml'))\n",
    "\n",
    "# Define pipeline\n",
    "@dsl.pipeline(\n",
    "    name='AgriAutoML Pipeline',\n",
    "    description='End-to-end pipeline for agricultural yield prediction'\n",
    ")\n",
    "def agri_automl_pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    vision_dataset_uri: str,\n",
    "    tabular_dataset_uri: str,\n",
    "    min_accuracy: float = 0.8\n",
    "):\n",
    "    # Preprocess data\n",
    "    preprocess_task = preprocess_data(\n",
    "        vision_data=vision_dataset_uri,\n",
    "        tabular_data=tabular_dataset_uri,\n",
    "        bucket_name=bucket_name\n",
    "    )\n",
    "\n",
    "    # Train vision model\n",
    "    train_vision_task = train_vision_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['vision_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_vision_task.after(preprocess_task)\n",
    "\n",
    "    # Train tabular model\n",
    "    train_tabular_task = train_tabular_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['tabular_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_tabular_task.after(preprocess_task)\n",
    "\n",
    "    # Deploy models\n",
    "    deploy_task = deploy_models(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        vision_model=train_vision_task.outputs['model_info'],\n",
    "        tabular_model=train_tabular_task.outputs['model_info']\n",
    "    )\n",
    "    deploy_task.after(train_vision_task, train_tabular_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "\n",
    "# Compile pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=agri_automl_pipeline,\n",
    "    package_path='pipeline.yaml'\n",
    ")\n",
    "\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name='agri-automl-pipeline',\n",
    "    template_path='pipeline.yaml',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,  # Changed from PROJECT_ID\n",
    "        'region': REGION,\n",
    "        'bucket_name': BUCKET_NAME,\n",
    "        'vision_dataset_uri': vision_uri,  # Changed from VISION_DATASET_URI\n",
    "        'tabular_dataset_uri': tabular_uri,  # Changed from TABULAR_DATASET_URI\n",
    "        'min_accuracy': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
