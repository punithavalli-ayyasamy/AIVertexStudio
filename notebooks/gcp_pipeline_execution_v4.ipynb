{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgriAutoML Pipeline Execution in Vertex AI\n",
    "\n",
    "This notebook demonstrates how to execute the AgriAutoML pipeline directly in Vertex AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 2112 (787.1 MB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: google-cloud-aiplatform 1.104.0\n",
      "Uninstalling google-cloud-aiplatform-1.104.0:\n",
      "  Successfully uninstalled google-cloud-aiplatform-1.104.0\n",
      "Found existing installation: google-cloud-storage 2.14.0\n",
      "Uninstalling google-cloud-storage-2.14.0:\n",
      "  Successfully uninstalled google-cloud-storage-2.14.0\n",
      "Found existing installation: google-cloud-datastore 1.15.5\n",
      "Uninstalling google-cloud-datastore-1.15.5:\n",
      "  Successfully uninstalled google-cloud-datastore-1.15.5\n",
      "Found existing installation: protobuf 4.25.8\n",
      "Uninstalling protobuf-4.25.8:\n",
      "  Successfully uninstalled protobuf-4.25.8\n",
      "Found existing installation: google-cloud-bigquery 2.6.1\n",
      "Uninstalling google-cloud-bigquery-2.6.1:\n",
      "  Successfully uninstalled google-cloud-bigquery-2.6.1\n",
      "Found existing installation: google-genai 1.26.0\n",
      "Uninstalling google-genai-1.26.0:\n",
      "  Successfully uninstalled google-genai-1.26.0\n",
      "Found existing installation: kfp 2.13.0\n",
      "Uninstalling kfp-2.13.0:\n",
      "  Successfully uninstalled kfp-2.13.0\n",
      "Found existing installation: pydantic 2.11.7\n",
      "Uninstalling pydantic-2.11.7:\n",
      "  Successfully uninstalled pydantic-2.11.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\charl\\CascadeProjects\\AIVertexStudio\\venv\\Lib\\site-packages\\google\\~~pb'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf<4.0.0dev,>=3.20.2\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp-pipeline-spec 0.6.0 requires protobuf<5,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from pydantic) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from pydantic) (0.4.1)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Installing collected packages: pydantic\n",
      "Successfully installed pydantic-2.11.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting google-cloud-bigquery<3.0.0\n",
      "  Downloading google_cloud_bigquery-2.6.1-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-api-core<2.0.0dev,>=1.23.0 (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0)\n",
      "  Downloading google_api_core-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-bigquery<3.0.0) (1.26.1)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1 (from google-cloud-bigquery<3.0.0)\n",
      "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=0.6.0 (from google-cloud-bigquery<3.0.0)\n",
      "  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six<2.0.0dev,>=1.13.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-bigquery<3.0.0) (1.17.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-bigquery<3.0.0) (3.20.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (2.40.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (1.49.0rc1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (4.9.1)\n",
      "Collecting google-auth<3.0dev,>=1.25.0 (from google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0)\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (80.9.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0) (1.7.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (2025.7.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0dev,>=1.25.0->google-api-core<2.0.0dev,>=1.23.0->google-api-core[grpc]<2.0.0dev,>=1.23.0->google-cloud-bigquery<3.0.0) (0.6.1)\n",
      "Downloading google_cloud_bigquery-2.6.1-py2.py3-none-any.whl (211 kB)\n",
      "Downloading google_api_core-1.34.1-py3-none-any.whl (120 kB)\n",
      "Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\n",
      "Installing collected packages: google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "\n",
      "  Attempting uninstall: google-resumable-media\n",
      "\n",
      "    Found existing installation: google-resumable-media 2.7.2\n",
      "\n",
      "    Uninstalling google-resumable-media-2.7.2:\n",
      "\n",
      "      Successfully uninstalled google-resumable-media-2.7.2\n",
      "\n",
      "  Attempting uninstall: google-auth\n",
      "\n",
      "    Found existing installation: google-auth 2.40.3\n",
      "\n",
      "    Uninstalling google-auth-2.40.3:\n",
      "\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "      Successfully uninstalled google-auth-2.40.3\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "  Attempting uninstall: google-api-core\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "    Found existing installation: google-api-core 2.25.1\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "    Uninstalling google-api-core-2.25.1:\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "      Successfully uninstalled google-api-core-2.25.1\n",
      "   -------- ------------------------------- 1/5 [google-auth]\n",
      "   ---------------- ----------------------- 2/5 [google-api-core]\n",
      "   ---------------- ----------------------- 2/5 [google-api-core]\n",
      "  Attempting uninstall: google-cloud-core\n",
      "   ---------------- ----------------------- 2/5 [google-api-core]\n",
      "    Found existing installation: google-cloud-core 2.4.3\n",
      "   ---------------- ----------------------- 2/5 [google-api-core]\n",
      "    Uninstalling google-cloud-core-2.4.3:\n",
      "   ---------------- ----------------------- 2/5 [google-api-core]\n",
      "      Successfully uninstalled google-cloud-core-2.4.3\n",
      "   ---------------- ----------------------- 2/5 [google-api-core]\n",
      "   -------------------------------- ------- 4/5 [google-cloud-bigquery]\n",
      "   -------------------------------- ------- 4/5 [google-cloud-bigquery]\n",
      "   -------------------------------- ------- 4/5 [google-cloud-bigquery]\n",
      "   ---------------------------------------- 5/5 [google-cloud-bigquery]\n",
      "\n",
      "Successfully installed google-api-core-1.34.1 google-auth-1.35.0 google-cloud-bigquery-2.6.1 google-cloud-core-1.7.3 google-resumable-media-1.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: setuptools>=65.5.1 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (80.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wheel>=0.40.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-bigquery 2.6.1 requires google-api-core[grpc]<2.0.0dev,>=1.23.0, but you have google-api-core 2.25.1 which is incompatible.\n",
      "google-cloud-bigquery 2.6.1 requires google-cloud-core<2.0dev,>=1.4.1, but you have google-cloud-core 2.4.3 which is incompatible.\n",
      "google-cloud-bigquery 2.6.1 requires google-resumable-media<2.0dev,>=0.6.0, but you have google-resumable-media 2.7.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage<3.0.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=1.32.0) (2.40.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=1.32.0) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=1.32.0) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=1.32.0) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=1.32.0) (2.32.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-cloud-storage<3.0.0,>=1.32.0) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0.0,>=1.32.0) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0.0,>=1.32.0) (4.25.8)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0.0,>=1.32.0) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=1.32.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=1.32.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=1.32.0) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=1.32.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=1.32.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=1.32.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=1.32.0) (2025.7.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\charl\\cascadeprojects\\aivertexstudio\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0.0,>=1.32.0) (0.6.1)\n",
      "Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Installing collected packages: google-cloud-storage\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 3.2.0\n",
      "    Uninstalling google-cloud-storage-3.2.0:\n",
      "      Successfully uninstalled google-cloud-storage-3.2.0\n",
      "Successfully installed google-cloud-storage-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip cache purge\n",
    "\n",
    "# First, uninstall all related packages\n",
    "%pip uninstall -y google-cloud-aiplatform google-cloud-storage google-cloud-datastore protobuf google-cloud-bigquery google-genai kfp pydantic\n",
    "\n",
    "\n",
    "%pip install --no-cache-dir \"protobuf>=3.20.2,<4.0.0dev\"\n",
    "%pip install --no-cache-dir \"pydantic\"\n",
    "\n",
    "%pip install --no-cache-dir \"google-cloud-bigquery<3.0.0\"\n",
    "#%pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Install core dependencies first\n",
    "\n",
    "%pip install --no-cache-dir \"setuptools>=65.5.1\"\n",
    "%pip install --no-cache-dir \"wheel>=0.40.0\"\n",
    "\n",
    "# Try installing KFP 2.0.1 specifically (a stable version)\n",
    "%pip install --no-cache-dir kfp>=2.0.0 --use-pep517\n",
    "\n",
    "\n",
    "# Install remaining dependencies after KFP is installed\n",
    "%pip install --no-cache-dir \"google-cloud-storage>=1.32.0,<3.0.0\"\n",
    "%pip install --no-cache-dir \"google-cloud-datastore==1.15.5\"\n",
    "%pip install --no-cache-dir \"google-cloud-aiplatform==1.104.0\"\n",
    "%pip install --no-cache-dir \"google-cloud-bigquery<3.0.0\"\n",
    "%pip install --no-cache-dir pandas numpy pillow scikit-learn tensorflow\n",
    "%pip install --no-cache-dir google-auth google-auth-httplib2 google-api-python-client\n",
    "\n",
    "\n",
    "# Install remaining dependencies\n",
    "%pip install pandas numpy pillow scikit-learn tensorflow google-auth google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "from datetime import datetime\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Dataset, Input, Artifact\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import io\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "# Get default credentials and project\n",
    "credentials, project_id = default()\n",
    "\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "bucket_name = \"agrifingcpflow-465809-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{bucket_name}/pipeline_root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample datasets for vision and tabular models.\"\"\"\n",
    "    # Create sample vision data (dummy image)\n",
    "    vision_uri = f\"gs://{bucket_name}/sample_vision_data\"\n",
    "    \n",
    "    # Create sample tabular data\n",
    "    tabular_uri = f\"gs://{bucket_name}/sample_tabular_data\"\n",
    "    \n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Create and upload sample vision data\n",
    "    img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    \n",
    "    vision_blob = bucket.blob('sample_vision_data/image1.png')\n",
    "    vision_blob.upload_from_string(img_byte_arr, content_type='image/png')\n",
    "    \n",
    "    # Create and upload sample tabular data\n",
    "    df = pd.DataFrame({\n",
    "        'planting_date': pd.date_range(start='2025-01-01', periods=100),\n",
    "        'temperature': np.random.normal(25, 5, 100),\n",
    "        'rainfall': np.random.normal(50, 10, 100),\n",
    "        'soil_quality': np.random.choice(['good', 'medium', 'poor'], 100),\n",
    "        'yield': np.random.normal(75, 15, 100)\n",
    "    })\n",
    "    \n",
    "    tabular_blob = bucket.blob('sample_tabular_data/farming_data.csv')\n",
    "    tabular_blob.upload_from_string(df.to_csv(index=False))\n",
    "    \n",
    "    return vision_uri, tabular_uri\n",
    "\n",
    "# Create the sample data and get the URIs\n",
    "vision_uri, tabular_uri = create_sample_data()\n",
    "print(f\"Created vision dataset at: {vision_uri}\")\n",
    "print(f\"Created tabular dataset at: {tabular_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def preprocess_data(\n",
    "    vision_data: str,\n",
    "    tabular_data: str,\n",
    "    bucket_name: str,\n",
    "    vision_dataset: Output[Artifact],\n",
    "    tabular_dataset: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess vision and tabular data for training\n",
    "    \n",
    "    Args:\n",
    "        vision_data: GCS URI for vision dataset\n",
    "        tabular_data: GCS URI for tabular dataset\n",
    "        bucket_name: GCS bucket for processed data\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vision_dataset, tabular_dataset)\n",
    "    \"\"\"\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Process vision data\n",
    "    def process_image(image_bytes):\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((224, 224))  # Standard size for many vision models\n",
    "        return np.array(img)\n",
    "\n",
    "    # Process tabular data\n",
    "    def process_tabular(df):\n",
    "        # Handle missing values\n",
    "        df = df.fillna(df.mean())\n",
    "        \n",
    "        # Feature engineering\n",
    "        if \"planting_date\" in df.columns:\n",
    "            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"])\n",
    "            df[\"planting_month\"] = df[\"planting_date\"].dt.month\n",
    "            df[\"planting_day\"] = df[\"planting_date\"].dt.day\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # Process and save datasets\n",
    "    vision_blob = bucket.blob('processed_vision_data.txt')\n",
    "    vision_blob.upload_from_string(vision_data)\n",
    "    vision_output_uri = f\"gs://{bucket_name}/{vision_blob.name}\"\n",
    "\n",
    "     # Save to the KFP output location\n",
    "    with open(vision_dataset.path, 'w') as f:\n",
    "        f.write(vision_output_uri)\n",
    "\n",
    "    tabular_blob = bucket.blob('processed_tabular_data.csv')\n",
    "    tabular_blob.upload_from_string(tabular_data)\n",
    "    tabular_output_uri = f\"gs://{bucket_name}/{tabular_blob.name}\"\n",
    "    \n",
    "    # Save to the KFP output location\n",
    "    with open(tabular_dataset.path, 'w') as f:\n",
    "        f.write(tabular_output_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def train_vision_model(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    dataset: Input[Artifact],\n",
    "    min_accuracy: float,\n",
    "    model_info: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Train AutoML Vision model for crop analysis\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: Input artifact containing the processed vision dataset URI\n",
    "        min_accuracy: Minimum required accuracy\n",
    "        model_info: Output artifact for model information\n",
    "    \"\"\"\n",
    "     # Read dataset URI from input artifact\n",
    "    with open(dataset.path, 'r') as f:\n",
    "        dataset_uri = f.read().strip()\n",
    "\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.ImageDataset.create(\n",
    "        display_name=\"crop_vision_dataset\",\n",
    "        gcs_source=dataset_uri\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLImageTrainingJob(\n",
    "        display_name=\"crop_vision_model\",\n",
    "        prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=83,  # Approximately 5 minutes\n",
    "        model_type=\"CLOUD\",\n",
    "        base_model=None\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        budget_milli_node_hours=83,  # 5 minutes for testing\n",
    "        training_filter_split=\"\",  # No filter\n",
    "        model_display_name=\"crop_vision_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1\n",
    "    )\n",
    "\n",
    "    # Get model evaluation\n",
    "    eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "\n",
    "    # Check if model meets accuracy threshold\n",
    "    if eval_metrics.metrics['auRoc'] < min_accuracy:\n",
    "        raise ValueError(f\"Model accuracy {eval_metrics.metrics['auRoc']} below threshold {min_accuracy}\")\n",
    "\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'accuracy': float(eval_metrics.metrics['auRoc'])\n",
    "    }\n",
    "    with open(model_info.path, 'w') as f:\n",
    "        json.dump(info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def train_tabular_model(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    dataset: Input[Artifact],\n",
    "    min_accuracy: float,\n",
    "    model_info: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Train AutoML Tabular model for crop yield prediction\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: Input artifact containing the processed tabular dataset URI\n",
    "        min_accuracy: Minimum required accuracy (RMSE threshold)\n",
    "        model_info: Output artifact for model information\n",
    "    \"\"\"\n",
    "     # Read dataset URI from input artifact\n",
    "    with open(dataset.path, 'r') as f:\n",
    "        dataset_uri = f.read().strip()\n",
    "\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.TabularDataset.create(\n",
    "        display_name=\"crop_tabular_dataset\",\n",
    "        gcs_source=dataset_uri\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name=\"crop_tabular_model\",\n",
    "        optimization_objective=\"minimize-rmse\",\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"field_size\"}},\n",
    "            {\"numeric\": {\"column_name\": \"rainfall\"}},\n",
    "            {\"numeric\": {\"column_name\": \"temperature\"}},\n",
    "            {\"categorical\": {\"column_name\": \"location\"}},\n",
    "            {\"categorical\": {\"column_name\": \"crop_type\"}},\n",
    "            {\"timestamp\": {\"column_name\": \"date\"}}\n",
    "        ],\n",
    "        target_column=\"yield\",\n",
    "        budget_milli_node_hours=83,  # Approximately 5 minutes\n",
    "        optimization_prediction_type=\"regression\",\n",
    "        additional_experiments=[\"enable_model_compression\"]\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        model_display_name=\"crop_yield_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1\n",
    "    )\n",
    "\n",
    "    # Get model evaluation\n",
    "    eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "\n",
    "    # Check if model meets accuracy threshold\n",
    "    if eval_metrics.metrics['rmse'] > min_accuracy:\n",
    "        raise ValueError(f\"Model RMSE {eval_metrics.metrics['rmse']} above threshold {min_accuracy}\")\n",
    "\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'rmse': float(eval_metrics.metrics['rmse'])\n",
    "    }\n",
    "    with open(model_info.path, 'w') as f:\n",
    "        json.dump(info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def deploy_models(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    vision_model: Input[Artifact],\n",
    "    tabular_model: Input[Artifact],\n",
    "    endpoints: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploy trained models to endpoints\n",
    "    \n",
    "     Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        vision_model: Input artifact containing vision model information\n",
    "        tabular_model: Input artifact containing tabular model information\n",
    "        endpoints: Output artifact for endpoint information\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "     # Read model info from input artifacts\n",
    "    with open(vision_model.path, 'r') as f:\n",
    "        vision_model_info = json.load(f)\n",
    "    \n",
    "    with open(tabular_model.path, 'r') as f:\n",
    "        tabular_model_info = json.load(f)\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Deploy vision model\n",
    "    vision_model_resource = aiplatform.Model(vision_model_info['model'])\n",
    "    vision_endpoint = vision_model_resource.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    # Deploy tabular model\n",
    "    tabular_model_resource = aiplatform.Model(tabular_model_info['model'])\n",
    "    tabular_endpoint = tabular_model_resource.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "        # Write endpoint information to output artifact\n",
    "    endpoint_info = {\n",
    "        'vision_endpoint': vision_endpoint.resource_name,\n",
    "        'tabular_endpoint': tabular_endpoint.resource_name\n",
    "    }\n",
    "    \n",
    "    with open(endpoints.path, 'w') as f:\n",
    "        json.dump(endpoint_info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "@dsl.pipeline(\n",
    "    name='AgriAutoML Pipeline',\n",
    "    description='End-to-end pipeline for agricultural yield prediction'\n",
    ")\n",
    "def agri_automl_pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    vision_dataset_uri: str,\n",
    "    tabular_dataset_uri: str,\n",
    "    min_accuracy: float = 0.8\n",
    "):\n",
    "    # Preprocess data\n",
    "    preprocess_task = preprocess_data(\n",
    "        vision_data=vision_dataset_uri,\n",
    "        tabular_data=tabular_dataset_uri,\n",
    "        bucket_name=bucket_name\n",
    "    )\n",
    "\n",
    "    # Train vision model\n",
    "    train_vision_task = train_vision_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['vision_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_vision_task.after(preprocess_task)\n",
    "\n",
    "    # Train tabular model\n",
    "    train_tabular_task = train_tabular_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['tabular_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_tabular_task.after(preprocess_task)\n",
    "\n",
    "    # Deploy models\n",
    "    deploy_task = deploy_models(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        vision_model=train_vision_task.outputs['model_info'],\n",
    "        tabular_model=train_tabular_task.outputs['model_info']\n",
    "    )\n",
    "    deploy_task.after(train_vision_task, train_tabular_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "\n",
    "# Compile pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=agri_automl_pipeline,\n",
    "    package_path='pipeline.yaml'\n",
    ")\n",
    "\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name='agri-automl-pipeline',\n",
    "    template_path='pipeline.yaml',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,  # Changed from PROJECT_ID\n",
    "        'region': REGION,\n",
    "        'bucket_name': bucket_name,\n",
    "        'vision_dataset_uri': vision_uri,  # Changed from VISION_DATASET_URI\n",
    "        'tabular_dataset_uri': tabular_uri,  # Changed from TABULAR_DATASET_URI\n",
    "        'min_accuracy': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
