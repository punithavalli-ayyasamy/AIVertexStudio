{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgriAutoML Pipeline Execution in Vertex AI\n",
    "\n",
    "This notebook demonstrates how to execute the AgriAutoML pipeline directly in Vertex AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 6 (1.4 MB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'xargs' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements-py310.txt'\n"
     ]
    }
   ],
   "source": [
    "# Clear pip cache and uninstall existing packages\n",
    "#%pip cache purge\n",
    "\n",
    "# First, uninstall all related packages\n",
    "%pip uninstall -y kfp kfp-server-api kfp-pipeline-spec protobuf google-cloud-aiplatform\n",
    "\n",
    "# Set the environment variable for protobuf\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# Install protobuf first with a compatible version\n",
    "%pip install --no-cache-dir protobuf==3.20.3\n",
    "%pip install --no-cache-dir kfp==1.8.22\n",
    "\n",
    "\n",
    "# Install Google Cloud packages\n",
    "%pip install --no-cache-dir google-cloud-aiplatform==1.35.0\n",
    "%pip install --no-cache-dir google-cloud-storage==2.14.0\n",
    "# Install specific versions that work together\n",
    "\n",
    "%pip install --no-cache-dir kfp-server-api==2.0.0\n",
    "%pip install --no-cache-dir kfp-pipeline-spec==0.2.2\n",
    "%pip install --no-cache-dir google-cloud-aiplatform==1.35.0\n",
    "%pip install --no-cache-dir google-cloud-storage==2.14.0\n",
    "\n",
    "# Install requests-toolbelt first with a compatible version\n",
    "%pip install --no-cache-dir requests-toolbelt==0.9.1\n",
    "\n",
    "# Install KFP with a version known to work with Python 3.10\n",
    "#%pip install --no-cache-dir kfp==1.8.22\n",
    "\n",
    "# Install google-cloud-aiplatform with a compatible version\n",
    "%pip install --no-cache-dir google-cloud-aiplatform==1.35.0\n",
    "\n",
    "\n",
    "\n",
    "# Core dependencies\n",
    "%pip install --no-cache-dir packaging==24.0\n",
    "\n",
    "%pip install --no-cache-dir pydantic==2.5.3\n",
    "%pip install --no-cache-dir setuptools==69.0.3\n",
    "%pip install --no-cache-dir wheel==0.42.0\n",
    "\n",
    "# GCP dependencies\n",
    "%pip install --no-cache-dir google-cloud-storage==2.14.0\n",
    "%pip install --no-cache-dir google-cloud-datastore==2.18.0\n",
    "\n",
    "%pip install --no-cache-dir \"google-cloud-bigquery[bqstorage,pandas]==3.13.0\"\n",
    "%pip install --no-cache-dir google-genai==0.3.0\n",
    "\n",
    "# ML Pipeline dependencies\n",
    "#%pip install --no-cache-dir kfp==2.0.1\n",
    "%pip install --no-cache-dir click==8.1.7\n",
    "\n",
    "%pip install --no-cache-dir kubernetes==28.1.0\n",
    "%pip install --no-cache-dir PyYAML==6.0.1\n",
    "\n",
    "%pip install --no-cache-dir tabulate==0.9.0\n",
    "\n",
    "# Data processing & ML\n",
    "%pip install --no-cache-dir pandas==2.1.4\n",
    "%pip install --no-cache-dir numpy==1.24.3\n",
    "%pip install --no-cache-dir pillow==10.2.0\n",
    "%pip install --no-cache-dir scikit-learn==1.3.2\n",
    "%pip install --no-cache-dir tensorflow==2.15.0\n",
    "\n",
    "# BigQuery and related\n",
    "%pip install --no-cache-dir db-dtypes==1.1.1\n",
    "%pip install --no-cache-dir bigframes==0.17.0\n",
    "%pip install --no-cache-dir pandas-gbq==0.19.2\n",
    "\n",
    "# Authentication\n",
    "%pip install --no-cache-dir google-auth==2.27.0\n",
    "%pip install --no-cache-dir google-auth-httplib2==0.1.1\n",
    "%pip install --no-cache-dir google-api-python-client==2.117.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "from datetime import datetime\n",
    "from kfp import dsl, compiler\n",
    "from kfp.dsl import Output, Dataset, Input, Artifact\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from components.pipeline_components import preprocess_data, train_tabular_model, deploy_model\n",
    "\n",
    "# Get default credentials and project\n",
    "credentials, project_id = default()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "bucket_name = \"qwiklabs-gcp-03-a9f5fc83bfac-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{bucket_name}/pipeline_root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample datasets for vision and tabular models.\"\"\"\n",
    "    # Create sample vision data (dummy image)\n",
    "    vision_uri = f\"gs://{bucket_name}/sample_vision_data\"\n",
    "    \n",
    "    # Create sample tabular data\n",
    "    tabular_uri = f\"gs://{bucket_name}/sample_tabular_data\"\n",
    "    \n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Create and upload sample vision data\n",
    "    img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    \n",
    "    vision_blob = bucket.blob('sample_vision_data/image1.png')\n",
    "    vision_blob.upload_from_string(img_byte_arr, content_type='image/png')\n",
    "    \n",
    "    # Create and upload sample tabular data\n",
    "    df = pd.DataFrame({\n",
    "        'planting_date': pd.date_range(start='2025-01-01', periods=100),\n",
    "        'temperature': np.random.normal(25, 5, 100),\n",
    "        'rainfall': np.random.normal(50, 10, 100),\n",
    "        'soil_quality': np.random.choice(['good', 'medium', 'poor'], 100),\n",
    "        'yield': np.random.normal(75, 15, 100)\n",
    "    })\n",
    "    \n",
    "    tabular_blob = bucket.blob('sample_tabular_data/farming_data.csv')\n",
    "    tabular_blob.upload_from_string(df.to_csv(index=False))\n",
    "    \n",
    "    return vision_uri, tabular_uri\n",
    "\n",
    "# Create the sample data and get the URIs\n",
    "vision_uri, tabular_uri = create_sample_data()\n",
    "print(f\"Created vision dataset at: {vision_uri}\")\n",
    "print(f\"Created tabular dataset at: {tabular_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'google-cloud-aiplatform==1.104.0',\n",
    "        'pandas',\n",
    "        'pillow',\n",
    "        'numpy'\n",
    "    ]\n",
    ")\n",
    "def preprocess_data(\n",
    "    tabular_data: str,\n",
    "    bucket_name: str,\n",
    "    project_id: str,  # Add project_id as parameter\n",
    "    region: str,    # Add region as parameter\n",
    "    tabular_dataset: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess vision and tabular data for training\n",
    "    \n",
    "    Args:\n",
    "        vision_data: GCS URI for vision dataset\n",
    "        tabular_data: GCS URI for tabular dataset\n",
    "        bucket_name: GCS bucket for processed data\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        vision_dataset: Output path for processed vision data\n",
    "        tabular_dataset: Output path for processed tabular data\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "       \n",
    "       \n",
    "    # Configure logging to output to both file and console\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(sys.stdout),  # Console output\n",
    "            logging.FileHandler('preprocess.log')  # File output\n",
    "        ]\n",
    "    )\n",
    "    log = logging.getLogger('preprocess_data')\n",
    "    # Initialize GCS client\n",
    "    log.info(f\"Initializing GCS client for project {project_id} in {region}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "   # Process vision data\n",
    "    def process_image(image_bytes):\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize((224, 224))  # Standard size for many vision models\n",
    "        return np.array(img)\n",
    "\n",
    "     # Process tabular data\n",
    "    def process_tabular(df):\n",
    "        # Handle missing values\n",
    "        df = df.fillna(df.mean())\n",
    "        \n",
    "        # Feature engineering\n",
    "        if \"planting_date\" in df.columns:\n",
    "            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"])\n",
    "            df[\"planting_month\"] = df[\"planting_date\"].dt.month\n",
    "            df[\"planting_day\"] = df[\"planting_date\"].dt.day\n",
    "        \n",
    "        return df\n",
    "   \n",
    "    log.info(\"Processing and uploading vision data\")\n",
    "    \n",
    "    df = pd.read_csv(io.StringIO(tabular_data))\n",
    "    df = process_tabular(df)\n",
    "\n",
    "    tabular_blob = bucket.blob('processed_tabular_data.csv')\n",
    "    #tabular_blob.upload_from_string(tabular_data)\n",
    "    tabular_blob.upload_from_string(df.to_csv(index=False))\n",
    "    tabular_output_uri = f\"gs://{bucket_name}/{tabular_blob.name}\"\n",
    "    \n",
    "    # Save to the KFP output location\n",
    "    with open(tabular_dataset.path, 'w') as f:\n",
    "        f.write(tabular_output_uri)\n",
    "    log.info(\"Preprocessing completed successfully\")\n",
    "    # Process and save datasets\n",
    "   \n",
    "\n",
    "\n",
    " \n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "         'google-cloud-aiplatform>=1.25.0',\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'xgboost',\n",
    "        'lightgbm',\n",
    "        'protobuf>=4.21.1,<5.0.0',  # Pin protobuf version for KFP compatibility\n",
    "        'click>=8.0.0,<9.0.0',\n",
    "        'kfp-server-api>=2.1.0,<2.5.0',\n",
    "        'kubernetes>=8.0.0,<31.0.0',\n",
    "        'PyYAML>=5.3,<7.0.0',\n",
    "        'requests-toolbelt>=0.8.0,<2.0.0',\n",
    "        'tabulate>=0.8.6,<1.0.0',\n",
    "        'kfp==2.13.0',  # Add KFP itself\n",
    "        'kfp-pipeline-spec==0.6.0',  # Add KFP pipeline spec\n",
    "        'packaging>=24.2.0',  # Add packaging with correct version\n",
    "        'google-cloud-bigquery[bqstorage,pandas]>=3.31.0,<4.0.0'\n",
    "    ]\n",
    ")\n",
    "def train_tabular_model(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    dataset: Input[Artifact],\n",
    "    min_accuracy: float,\n",
    "    model_info: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Train AutoML Tabular model for crop yield prediction\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        dataset: Input artifact containing the processed tabular dataset URI\n",
    "        min_accuracy: Minimum required accuracy (RMSE threshold)\n",
    "        model_info: Output artifact for model information\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from google.cloud import aiplatform\n",
    "    import sys\n",
    "    \n",
    "    \n",
    "     # Read dataset URI from input artifact\n",
    "    with open(dataset.path, 'r') as f:\n",
    "        dataset_uri = f.read().strip()\n",
    "\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Create dataset\n",
    "    ai_dataset = aiplatform.TabularDataset.create(\n",
    "        display_name=\"crop_tabular_dataset\",\n",
    "        gcs_source=dataset_uri\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name=\"crop_tabular_model\",\n",
    "        optimization_objective=\"minimize-rmse\",\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"field_size\"}},\n",
    "            {\"numeric\": {\"column_name\": \"rainfall\"}},\n",
    "            {\"numeric\": {\"column_name\": \"temperature\"}},\n",
    "            {\"categorical\": {\"column_name\": \"location\"}},\n",
    "            {\"categorical\": {\"column_name\": \"crop_type\"}},\n",
    "            {\"timestamp\": {\"column_name\": \"date\"}}\n",
    "        ],\n",
    "        optimization_prediction_type=\"regression\"\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    ai_model = job.run(\n",
    "        dataset=ai_dataset,\n",
    "        target_column=\"yield\",\n",
    "        model_display_name=\"crop_yield_model\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1,\n",
    "        budget_milli_node_hours=1000,\n",
    "    )\n",
    "\n",
    "    # Get model evaluation\n",
    "   # eval_metrics = ai_model.list_model_evaluations()[0]\n",
    "\n",
    "    # Check if model meets accuracy threshold\n",
    "    #if eval_metrics.metrics['rmse'] > min_accuracy:\n",
    "     #   raise ValueError(f\"Model RMSE {eval_metrics.metrics['rmse']} above threshold {min_accuracy}\")\n",
    "\n",
    "    # Return model info\n",
    "    model_info = {\n",
    "        'model': ai_model.resource_name,\n",
    "        'rmse': float(eval_metrics.metrics['rmse'])\n",
    "    }\n",
    "    with open(model_info.path, 'w') as f:\n",
    "        json.dump(info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform>=1.25.0',\n",
    "        'google-cloud-storage>=1.32.0,<3.0.0',\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'protobuf<4.0.0dev,>=3.19.5'  # Often needed for compatibility\n",
    "    ]\n",
    ")\n",
    "def deploy_models(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    vision_model: Input[Artifact],\n",
    "    tabular_model: Input[Artifact],\n",
    "    endpoints: Output[Artifact]\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploy trained models to endpoints\n",
    "    \n",
    "     Args:\n",
    "        project_id: GCP project ID\n",
    "        region: GCP region\n",
    "        vision_model: Input artifact containing vision model information\n",
    "        tabular_model: Input artifact containing tabular model information\n",
    "        endpoints: Output artifact for endpoint information\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import Model\n",
    "    import time\n",
    "\n",
    "     # Read model info from input artifacts\n",
    "    with open(vision_model.path, 'r') as f:\n",
    "        vision_model_info = json.load(f)\n",
    "    \n",
    "    with open(tabular_model.path, 'r') as f:\n",
    "        tabular_model_info = json.load(f)\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    # Deploy vision model\n",
    "    vision_model_resource = aiplatform.Model(vision_model_info['model'])\n",
    "    vision_endpoint = vision_model_resource.deploy(\n",
    "        machine_type='e2-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    # Deploy tabular model\n",
    "    tabular_model_resource = aiplatform.Model(tabular_model_info['model'])\n",
    "    tabular_endpoint = tabular_model_resource.deploy(\n",
    "        machine_type='e2-standard-4',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "\n",
    "    # Write endpoint information to output artifact\n",
    "    endpoint_info = {\n",
    "        'vision_endpoint': vision_endpoint.resource_name,\n",
    "        'tabular_endpoint': tabular_endpoint.resource_name\n",
    "    }\n",
    "    \n",
    "    with open(endpoints.path, 'w') as f:\n",
    "        json.dump(endpoint_info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "@dsl.pipeline(\n",
    "    name='AgriAutoML Pipeline',\n",
    "    description='End-to-end pipeline for agricultural yield prediction'\n",
    ")\n",
    "def agri_automl_pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    tabular_dataset_uri: str,\n",
    "    min_accuracy: float = 0.8\n",
    "):\n",
    "    # Preprocess data\n",
    "    preprocess_task = preprocess_data(\n",
    "        tabular_data=tabular_dataset_uri,\n",
    "        bucket_name=bucket_name,\n",
    "        project_id=project_id,  # Add this\n",
    "        region=region \n",
    "    )\n",
    "\n",
    "\n",
    "    # Train tabular model\n",
    "    train_tabular_task = train_tabular_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['tabular_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "        model_display_name='agri-yield-predictor'\n",
    "    )\n",
    "    train_tabular_task.after(preprocess_task)\n",
    "\n",
    "    # Deploy model\n",
    "    deploy_task = deploy_models(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        tabular_model=train_tabular_task.outputs['model_info'],\n",
    "        endpoint_name='agri-yield-endpoint'\n",
    "    )\n",
    "    deploy_task.after(train_tabular_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GCP setup\n",
    "print(f\"Current Project ID: {project_id}\")\n",
    "print(f\"Current Region: {REGION}\")\n",
    "print(\"Authenticated as:\", credentials.service_account_email if hasattr(credentials, 'service_account_email') else \"User Account\")\n",
    "\n",
    "# Test GCP API access\n",
    "storage_client = storage.Client()\n",
    "try:\n",
    "    # List buckets to test access\n",
    "    buckets = list(storage_client.list_buckets(max_results=1))\n",
    "    print(\"✅ Storage API access successful\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Storage API access failed:\", str(e))\n",
    "\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "\n",
    "# Compile pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=agri_automl_pipeline,\n",
    "    package_path='pipeline.yaml'\n",
    ")\n",
    "\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name='agri-automl-pipeline',\n",
    "    template_path='pipeline.yaml',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,  # Changed from PROJECT_ID\n",
    "        'region': REGION,\n",
    "        'bucket_name': bucket_name,\n",
    "        'tabular_dataset_uri': tabular_uri,  # Changed from TABULAR_DATASET_URI\n",
    "        'min_accuracy': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
