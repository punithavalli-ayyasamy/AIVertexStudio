{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Yield Prediction Pipeline in Vertex AI\n",
    "\n",
    "This notebook demonstrates the deployment of a tabular model for crop yield prediction using Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip cache purge\n",
    "\n",
    "# Install core dependencies\n",
    "%pip install --no-cache-dir \"setuptools>=65.5.1\"\n",
    "%pip install --no-cache-dir \"wheel>=0.40.0\"\n",
    "%pip install --no-cache-dir \"protobuf>=4.21.1,<5.0.0\"\n",
    "%pip install --no-cache-dir \"pydantic>=2.0.0,<3.0.0\"\n",
    "\n",
    "# Install GCP dependencies\n",
    "%pip install --no-cache-dir \"google-cloud-aiplatform==1.104.0\"\n",
    "%pip install --no-cache-dir \"google-cloud-storage>=2.0.0\"\n",
    "%pip install --no-cache-dir 'google-cloud-bigquery[bqstorage,pandas]>=3.31.0,<4.0.0'\n",
    "\n",
    "# Install KFP and related packages\n",
    "%pip install --no-cache-dir kfp>=2.0.0 --use-pep517\n",
    "%pip install --no-cache-dir 'kfp-pipeline-spec==0.6.0'\n",
    "\n",
    "# Install data processing packages\n",
    "%pip install --no-cache-dir pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "from datetime import datetime\n",
    "from kfp import dsl, compiler\n",
    "from kfp.dsl import Output, Dataset, Input, Artifact\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get default credentials and project\n",
    "credentials, project_id = default()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "bucket_name = \"agrifingcpflow-465809-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{bucket_name}/pipeline_root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data():\n",
    "    \"\"\"Create sample tabular dataset for crop yield prediction.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Create sample tabular data\n",
    "    df = pd.DataFrame({\n",
    "        'field_size': np.random.uniform(1, 100, 100),\n",
    "        'temperature': np.random.normal(25, 5, 100),\n",
    "        'rainfall': np.random.normal(50, 10, 100),\n",
    "        'soil_quality': np.random.choice(['good', 'medium', 'poor'], 100),\n",
    "        'yield': np.random.normal(75, 15, 100)\n",
    "    })\n",
    "    \n",
    "    # Upload to GCS\n",
    "    blob = bucket.blob('sample_tabular_data/farming_data.csv')\n",
    "    blob.upload_from_string(df.to_csv(index=False))\n",
    "    \n",
    "    return f\"gs://{bucket_name}/sample_tabular_data/farming_data.csv\"\n",
    "\n",
    "# Create the sample data and get the URI\n",
    "tabular_uri = create_sample_data()\n",
    "print(f\"Created tabular dataset at: {tabular_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-storage>=2.0.0',\n",
    "        'google-cloud-aiplatform==1.104.0',\n",
    "        'pandas',\n",
    "        'scikit-learn'\n",
    "    ]\n",
    ")\n",
    "def preprocess_data(\n",
    "    tabular_data: str,\n",
    "    bucket_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    tabular_dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Preprocess tabular data for crop yield prediction.\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Read data\n",
    "    df = pd.read_csv(tabular_data)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    df['soil_quality'] = le.fit_transform(df['soil_quality'])\n",
    "    \n",
    "    # Save processed data\n",
    "    output_uri = f\"gs://{bucket_name}/processed_data/farming_data_processed.csv\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob('processed_data/farming_data_processed.csv')\n",
    "    blob.upload_from_string(df.to_csv(index=False))\n",
    "    \n",
    "    # Save to the KFP output location\n",
    "    with open(tabular_dataset.path, 'w') as f:\n",
    "        f.write(output_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Crop Yield Prediction Pipeline',\n",
    "    description='Pipeline for agricultural yield prediction using tabular data'\n",
    ")\n",
    "def crop_prediction_pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    tabular_dataset_uri: str,\n",
    "    min_accuracy: float = 0.8\n",
    "):\n",
    "    # Preprocess data\n",
    "    preprocess_task = preprocess_data(\n",
    "        tabular_data=tabular_dataset_uri,\n",
    "        bucket_name=bucket_name,\n",
    "        project_id=project_id,\n",
    "        region=region\n",
    "    )\n",
    "\n",
    "    # Train tabular model\n",
    "    train_tabular_task = train_tabular_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        dataset=preprocess_task.outputs['tabular_dataset'],\n",
    "        min_accuracy=min_accuracy\n",
    "    )\n",
    "    train_tabular_task.after(preprocess_task)\n",
    "\n",
    "    # Deploy model\n",
    "    deploy_task = deploy_model(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        model=train_tabular_task.outputs['model_info']\n",
    "    )\n",
    "    deploy_task.after(train_tabular_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GCP setup\n",
    "print(f\"Current Project ID: {project_id}\")\n",
    "print(f\"Current Region: {REGION}\")\n",
    "print(\"Authenticated as:\", credentials.service_account_email if hasattr(credentials, 'service_account_email') else \"User Account\")\n",
    "\n",
    "# Test GCP API access\n",
    "storage_client = storage.Client()\n",
    "try:\n",
    "    buckets = list(storage_client.list_buckets(max_results=1))\n",
    "    print(\"✅ Storage API access successful\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Storage API access failed:\", str(e))\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=project_id,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "# Compile pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=crop_prediction_pipeline,\n",
    "    package_path='pipeline.yaml'\n",
    ")\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name='crop-yield-prediction-pipeline',\n",
    "    template_path='pipeline.yaml',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,\n",
    "        'region': REGION,\n",
    "        'bucket_name': bucket_name,\n",
    "        'tabular_dataset_uri': tabular_uri,\n",
    "        'min_accuracy': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
